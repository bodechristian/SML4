{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Investigation on neural networks\n",
    "\n",
    "In this notebook we explore several properties of neural networks and the influence of some parameters on the overall performance.\n",
    "You are not allowed to import any other library than the ones already imported. We only ask you to add some lines of code, removing or changing the given code is not allowed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EPOCHS = 15\n",
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = ...\n",
    "N_SAMPLES = ...\n",
    "USE_CONVOLUTIONS = ...\n",
    "RELU_AS_ACTIVATION = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 1: fill the \"TO DO\" lines to load the given dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "\n",
    "train_images = \"TO DO\"\n",
    "train_labels = \"TO DO\"\n",
    "test_images = \"TO DO\"\n",
    "test_labels = \"TO DO\"\n",
    "n_samples = train_images.shape[0]\n",
    "\n",
    "if USE_CONVOLUTIONS:\n",
    "    train_images = train_images.reshape((-1, 28, 28))\n",
    "    test_images = test_images.reshape((-1, 28, 28))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plots of some samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 2: plot the 9 first samples of the dataset on a 3 x 3 grid. The title of each subplot is the label of the image shown in the subplot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(3, 3)\n",
    "\n",
    "\"TO DO\"\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural network implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 3: Implement a Multi-Layer Perceptron (MLP) with the following architecture: Linear 512 - * - Linear 512 - * - Linear 512 - * - Linear 10, where * is ReLU if relu_as_activation is True and Tanh otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import flax.linen as nn\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    \"\"\"A simple MLP model.\"\"\"\n",
    "    relu_as_activation: bool\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        # make sure x is a batch \n",
    "        x = jnp.atleast_2d(x)\n",
    "        \"TO DO\"      \n",
    "        x = nn.Dense(features=10)(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 4: Implement a Convolutional Neural Network (CNN) with the following architecture: Conv 32 - * - AvgPool - Conv 64 - * - AvgPool - Linear 256 - * - Linear 10, where * is ReLU if relu_as_activation is True and Tanh otherwise and AvgPool is an average pooling layer. The kernel size for the convolutional layers is 3 x 3, the rest of the parameters are left unchanged from the default implementation of flax. The window size of the average pooling layers is 2 x 2 and the stride is 2 x 2, the rest of the parameters are left unchanged from the default implementation of flax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    \"\"\"A simple CNN model.\"\"\"\n",
    "    relu_as_activation: bool\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        # make sure x is a batch \n",
    "        x = jnp.atleast_3d(x)\n",
    "        # nn.Conv uses channel last convention\n",
    "        x = x[..., jnp.newaxis]\n",
    "        \"TO DO\"\n",
    "        x = nn.Dense(features=10)(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 5: Implement the model that we will train in the following cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "import jax\n",
    "import optax\n",
    "\n",
    "\n",
    "class Model:\n",
    "    def __init__(self, network, key, learning_rate, batch_size) -> None:\n",
    "        self.network = network \n",
    "        if type(self.network) == CNN:\n",
    "            self.params = self.network.init(key, x=\"TO DO\")\n",
    "        else:\n",
    "            self.params = self.network.init(key, x=\"TO DO\")\n",
    "\n",
    "        self.optimizer = optax.adam(learning_rate)\n",
    "        self.optimizer_state = self.optimizer.init(\"TO DO\")\n",
    "\n",
    "    # @partial(jax.jit, static_argnames=\"self\")\n",
    "    def update_model(self, params, batch, optimizer_state):\n",
    "        \"\"\"\n",
    "        Performs one gradient step on the batch given as input.\n",
    "        \"\"\"\n",
    "        loss, grad_loss = \"TO DO\"\n",
    "        updates, optimizer_state = \"TO DO\"\n",
    "        params = \"TO DO\"\n",
    "\n",
    "        return params, optimizer_state, loss, grad_loss\n",
    "\n",
    "    # @partial(jax.jit, static_argnames=\"self\")\n",
    "    def loss(self, params, batch) -> float:\n",
    "        \"\"\"\n",
    "        Compute the softmax cross entropy and average it over the batch given as input.\n",
    "        \"\"\"\n",
    "\n",
    "        return \"TO DO\"\n",
    "\n",
    "    # @partial(jax.jit, static_argnames=\"self\")\n",
    "    def accuracy(self, params, batch) -> float:\n",
    "        \"\"\"\n",
    "        Compute the accuracy of the model and average it over the batch given as input.\n",
    "        \"\"\"\n",
    "\n",
    "        return \"TO DO\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch_indexes(key, n_samples, batch_size):\n",
    "    perms = jax.random.permutation(key, n_samples)\n",
    "    perms = perms[: n_samples // batch_size * batch_size]  # skip incomplete batch\n",
    "    return perms.reshape((-1, batch_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 6: Write a script to train an instance of the class Model on the given dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np \n",
    "\n",
    "\n",
    "key = jax.random.PRNGKey(0)  # for deterministic randomness purposes\n",
    "\n",
    "model = Model(CNN() if USE_CONVOLUTIONS else MLP(), key, LEARNING_RATE, BATCH_SIZE)\n",
    "\n",
    "training_loss = np.zeros(N_EPOCHS)\n",
    "test_loss = np.zeros(N_EPOCHS)\n",
    "training_accuracy = np.zeros(N_EPOCHS)\n",
    "test_accuracy = np.zeros(N_EPOCHS)\n",
    "grad_norm_first_layer = np.zeros(N_EPOCHS)\n",
    "grad_norm_last_layer = np.zeros(N_EPOCHS)\n",
    "\n",
    "print(\"\\nStarting training...\")\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch_idx in tqdm(range(N_EPOCHS)):\n",
    "\tkey, _ = jax.random.split(key)\n",
    "\tpermutations = get_batch_indexes(key, n_samples, BATCH_SIZE)\n",
    "\n",
    "\tfor batch_indexes in tqdm(permutations, leave=False):\n",
    "\t\tbatch = \"TO DO\"\n",
    "\t\n",
    "\t\tmodel.params, model.optimizer_state, loss, grad_loss = model.update_model(model.params, batch, model.optimizer_state)\n",
    "\n",
    "\t\ttraining_loss[epoch_idx] += loss\n",
    "\t\ttraining_accuracy[epoch_idx] += model.accuracy(model.params, batch)\n",
    "\t\t\n",
    "\t\tgrad_loss_norm = jax.tree_map(jnp.linalg.norm, grad_loss)\n",
    "\t\tgrad_norm_first_layer[epoch_idx] += grad_loss_norm[\"params\"][\"Conv_0\" if USE_CONVOLUTIONS else \"Dense_0\"][\"kernel\"]\n",
    "\t\tgrad_norm_last_layer[epoch_idx] += grad_loss_norm[\"params\"][\"Dense_1\" if USE_CONVOLUTIONS else \"Dense_3\"][\"kernel\"]\n",
    "\t\t\n",
    "\n",
    "\ttraining_loss[epoch_idx] /= len(permutations)\n",
    "\ttraining_accuracy[epoch_idx] /= len(permutations)\n",
    "\tgrad_norm_first_layer[epoch_idx] /= len(permutations)\n",
    "\tgrad_norm_last_layer[epoch_idx] /= len(permutations)\n",
    "\ttest_loss[epoch_idx] = model.loss(model.params, (test_images, test_labels))\n",
    "\ttest_accuracy[epoch_idx] = model.accuracy(model.params, (test_images, test_labels))\n",
    "\n",
    "\n",
    "print(\"Training time:\", jnp.round(time.time() - start_time), \"s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plots of the training and testing metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2)\n",
    "\n",
    "ax[0].plot(training_loss, label=\"Train\")\n",
    "ax[0].plot(test_loss, label=\"Test\")\n",
    "ax[0].legend()\n",
    "ax[0].set_title(\"Loss\")\n",
    "ax[1].plot(training_accuracy, label=\"Train\")\n",
    "ax[1].plot(test_accuracy, label=\"Test\")\n",
    "ax[1].legend()\n",
    "_ = ax[1].set_title(\"Accuracy\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plots of the first missclassified sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 7: Plot the first miss-classified sample of the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import jax.numpy as jnp\n",
    "\n",
    "idx_first_error = \"TO DO\"\n",
    "prediction_first_error = \"TO DO\"\n",
    "label_first_error = \"TO DO\"\n",
    "\n",
    "plt.imshow(test_images[idx_first_error].reshape(28, 28), cmap='gray')\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.title(f\"Miss classified digit, prediction: {prediction_first_error}, true label: {label_first_error}\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store gradient norms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network_type = \"cnn\" if USE_CONVOLUTIONS else \"mlp\"\n",
    "activation_type = \"_relu\" if RELU_AS_ACTIVATION else \"_tanh\"\n",
    "\n",
    "np.save(\"grad_norm_first_layer_\" + network_type + activation_type, grad_norm_first_layer)\n",
    "np.save(\"grad_norm_last_layer_\" + network_type + activation_type, grad_norm_last_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot gradient norms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN_BOTH_ACTIVATIONS = False\n",
    "\n",
    "if RUN_BOTH_ACTIVATIONS:\n",
    "    grad_norm_first_layer_relu = jnp.load(f\"grad_norm_first_layer_{network_type}_relu.npy\")\n",
    "    grad_norm_last_layer_relu = jnp.load(f\"grad_norm_last_layer_{network_type}_relu.npy\")\n",
    "    grad_norm_first_layer_tanh = jnp.load(f\"grad_norm_first_layer_{network_type}_tanh.npy\")\n",
    "    grad_norm_last_layer_tanh = jnp.load(f\"grad_norm_last_layer_{network_type}_tanh.npy\")\n",
    "\n",
    "\n",
    "    plt.plot(grad_norm_first_layer_relu, label=\"First layer ReLU\")\n",
    "    plt.plot(grad_norm_last_layer_relu, label=\"Last layer ReLU\")\n",
    "    plt.plot(grad_norm_first_layer_tanh, label=\"First layer tanh\")\n",
    "    plt.plot(grad_norm_last_layer_tanh, label=\"Last layer tanh\")\n",
    "\n",
    "    plt.legend()\n",
    "    plt.title(f\"Gradients norm of the {network_type}\")\n",
    "    plt.tight_layout()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SML4",
   "language": "python",
   "name": "sml4"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
